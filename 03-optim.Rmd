# 优化算法 {#optim}


## 介绍 {#optim-intro}




## 牛顿优化算法


假设函数 $f(x)$ 可导并一阶导 $f^{\prime}(x)$ 连续，那么最大化 $f(x)$ 相当于求 $f^{\prime}(x) = 0$ 的根，我们就可以使用牛顿法求根。优化 $f(x)$ 就转化成了方程求根问题。 牛顿优化算法的迭代过程如下。

\centering
```{block, type='method'}
**牛顿优化算法**


1. 选择初始猜测点 $x_0$，设置 $n = 0$。
2. 按照以下迭代过程进行迭代：
\begin{equation}
x_{n+1}=x_{n}-\frac{f^{\prime}\left(x_{n}\right)}{f^{\prime\prime}\left(x_{n}\right)}.
\end{equation}
3. 计算 $|f^{\prime}(x_{n+1})|$。
    1. 如果 $|f^{\prime}(x_{n+1})| \leq \epsilon$，停止迭代；
    2. 否则，返回第 2 步。
```

上述优化过程的停止条件还可以为：

- $|x_{n} - x_{n-1}| < \epsilon$.

- $|f(x_{n}) - f(x_{n-1})| < \epsilon$.

## 多元优化算法

我们现在考虑多元优化算法，假设 $f: \mathbb{R}^d \rightarrow \mathbb{R}$，且 $f$ 的一阶和二阶偏导连续可导。我们记 $\mathbf{x} = (x_1, x_2, \cdots, x_d)^T$，那么我们有梯度向量
$$\nabla f(\mathbf x) = \frac{\partial f({\mathbf{x}})}{\partial {\mathbf{x}}} = 
    \left(
      \begin{array}{c}
        \frac{\partial f(\mathbf x)}{\partial x_1}\\
        \frac{\partial f(\mathbf x)}{\partial x_2}\\
        \vdots\\
        \frac{\partial f(\mathbf x)}{\partial x_d}
      \end{array}
    \right).$$
以及海塞矩阵
$$\mathbf{H}(\mathbf x)= \frac{\partial^2 f({ \mathbf{x}})}{\partial {\mathbf{x}}\partial{\mathbf{x}}'}= \left(\begin{array}{cccc}\frac{\partial^{2} f(\mathbf x)}{\partial x_{1}^{2}} & \frac{\partial^{2} f(\mathbf x)}{\partial x_{1} \partial x_{2}} & \cdots & \frac{\partial^{2} f(\mathbf x)}{\partial x_{1} \partial x_{d}} \\ \frac{\partial^{2} f(\mathbf x)}{\partial x_{2} \partial x_{1}} & \frac{\partial^{2} f(\mathbf x)}{\partial x_{2}^{2}} & \cdots & \frac{\partial^{2} f(\mathbf x)}{\partial x_{2} \partial x_{d}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^{2} f(\mathbf x)}{\partial x_{d} \partial x_{1}} & \frac{\partial^{2} f(\mathbf x)}{\partial x_{d} \partial x_{2}} & \cdots & \frac{\partial^{2} f(\mathbf x)}{\partial x_{d}^{2}}\end{array}\right).$$
和一元优化问题类似，我们可以通过迭代求 $f(\mathbf x)$ 的极值。比如多元的牛顿优化算法的迭代过程可以写为：
$$\mathbf{x_{n+1}}=\mathbf{x_n}-\left(\frac{\partial^2 f({ \mathbf{x}})}{\partial {\mathbf{x}}\partial{\mathbf{x}}'}\right)^{-1}\frac{\partial f({\mathbf{x}})}{\partial {\mathbf{x}}}.$$
停止条件可以为：

1. $||\nabla f(\mathbf x_n)|| < \epsilon$.

2. $||\mathbf x_n - \mathbf x_{n-1}|| < \epsilon$.

3. $|f(\mathbf x_n) - f(\mathbf x_{n-1})|| < \epsilon$.
                                                                                                

```{example}
通过牛顿优化算法求 $f(x_1, x_2) = x_1^2 - x_1x_2 + x_2^2 + \exp(x_2)$ 的最小值。
```

```{r newton}
newton <- function(f, x0, tol = 1e-9, max.iter = 100) {
  x <- x0
  cat(paste0('初始值：','x1 = ',x[1],', x2 = ',x[2],'\n'))
  fx <- ftn(f, x)
  iter <- 0
  # xs用来保存每步迭代得到的x值
  xs <- list()
  xs[[1]] <- x
  # 继续迭代直到满足停止条件
  while((max(abs(fx$fgrad)) > tol) & (iter < max.iter)){
    x <- x - solve(fx$fhess, fx$fgrad)
    fx <- ftn(f, x)
    iter <- iter + 1
    xs[[iter + 1]] <- x
    cat(paste0('迭代第',iter,'次：x1 = ',x[1],', x2 = ',x[2],'\n'))
  }
  if (max(abs(fx$fgrad)) > tol){
    cat('算法无法收敛 \n')
  } else{
    cat('算法收敛\n')
    return(xs)
  }
}

ftn <- function(f, x){
  df <- deriv(body(f), c('x1', 'x2'), func = TRUE, hessian = TRUE)
  dfx <- df(x[1], x[2])
  f <- dfx[1]
  fgrad <- attr(dfx, 'gradient')[1,]
  fhess <- attr(dfx, 'hessian')[1,,]
  return(list(f = f, fgrad = fgrad, fhess = fhess))
}
f <- function(x1, x2) x1^2 - x1 * x2 + x2^2 + exp(x2)
```

```{r, echo=FALSE, fig.asp=1}
m <- as.data.frame(do.call(rbind, newton(f, c(5, 5))))
nx <- ny <- 100
xg <- seq(-5, 5, len = nx)
yg <- seq(-5, 5, len = ny)
g <- expand.grid(xg, yg)
z <- f(g[,1], g[,2])
f_long <- data.frame(x = g[,1], y = g[,2], z = z)
library(ggplot2)
cggplot(f_long, aes(x, y, z = z)) + 
  geom_contour_filled(aes(fill = stat(level)), bins = 50) + 
  guides(fill = FALSE) +
  geom_path(data = m, aes(x1, x2, z=0), col = 2, arrow = arrow()) +
  geom_point(data = m, aes(x1, x2, z=0), size = 3, col = 2) +
  xlab(expression(x[1])) +
  ylab(expression(x[2])) +
  ggtitle(parse(text = paste0('"牛顿优化算法： "', ' ~ f(x[1],x[2]) == ~ x[1]^2 - x[1] * x[2] + x[2]^2 + exp(x[2])'))) 
```




## Nelder Mead 算法

在很多实际应用问题中，待优化的函数不可导，这就需要不依赖于求导的优化算法，Nelder Mead 算法，是最常用的不依赖于求导的算法之一，也是 R 中 `optim()` 函数的默认算法。

Nelder Mead 算法的思路是计算目标函数 $f(\cdot)$ 在 $n$ 维单纯形顶点出的取值，其中 $n$ 为 $f(\cdot)$ 中的变量个数，所以 Nelder Mead 算法有时也被称为单纯形优化法。对于一个二维函数，单纯形即为三角形，三角形的每一个角是一个顶点。更一般的，$n$ 维单纯形有 $n + 1$ 个顶点。

图 \@ref(fig:simplex1) 展示了一个二维单纯形的例子。

```{r simplex1, fig.cap = "二维单纯形示例", echo=FALSE}
t0 <- data.frame(x1 = c(0.5, 1.5, 1), x2 = c(1, -1, 3))
s0 <- ggplot(data = t0, aes(x = x1, y = x2)) +
  geom_point(size = 2, color = 'red') + 
  geom_polygon(col = 'red', fill = NA) +
  ggtitle("") +
  xlab(expression(x[1])) +
  ylab(expression(x[2])) +
  xlim(c(0, 2)) + 
  ylim(c(-6,6)) 
s0
```

#### 第一步：求函数值 {-}

对每个单纯形的顶点 ${\mathbf x_j}$，计算函数值 $f({\mathbf x_j})$，其中 $j \in 1, 2, \cdots, n$，并将将所有顶点排序使得 $$f({\mathbf x_1})\leq f({\mathbf x_2})\leq\ldots\leq f({\mathbf x_{n+1}}).$$假设我们的目标是最小化 $f(\cdot)$，那么 $f({\mathbf x_{n+1}})$ 就是最差点。算法目标即使寻找更好的点来取代  $f({\mathbf x_{n+1}})$。

#### 第二步：计算中心点 {-}

去掉最差点 ${\mathbf x_{n+1}}$，计算剩下的 $n$ 个点的中心点，$${\mathbf x_0}=\frac{1}{n}\sum_{j=1}^{n}  {\mathbf x_j}.$$
对于一个二维单纯形，中点即为两点连线的中点。


#### 第三步：计算映射点 {-}


根据中点计算最差点的映射，得到映射点（Reflection Point）：
$${\mathbf x_r}={\mathbf x_0}+\alpha({\mathbf x_0}-{\mathbf x_{n+1}}),$$

通常取 $\alpha=1$，这种情况下映射点和最差点到中点的距离相等。算法的下一步则取决于 $f({\mathbf x_r})$  的取值大小。

1. $f({\mathbf x_1})\leq f({\mathbf x_r})<f({\mathbf x_n})$， 这种情况下 ${\mathbf x_r}$ 既不是最佳点也不是最差点，将 $\mathbf x_{n+1}$ 替换为 ${\mathbf x_r}$，形成新的单纯形，回到第一步。
    
2. $f({\mathbf x_r})<f({\mathbf x_1})$，即 ${\mathbf x_r}$ 为最佳点，那么在这个方向上继续延伸，得到延伸点（Expansion Point）：
$$ {\mathbf x_e}={\mathbf x_0}+\gamma({\mathbf x_r}-{\mathbf x_0})，$$ 通常取 $\gamma = 2$。计算  $f({\mathbf x_e})$。
    - 如果 $f({\mathbf x_e}) < f({\mathbf x_r})$，那么延伸点比映射点更优，用延伸点 ${\mathbf x_e}$ 形成新的单纯形。
    - 如果 $f({\mathbf x_r})\leq f({\mathbf x_e})$，用映射点 ${\mathbf x_r}$ 形成新的单纯形。
    


    
3. $f({\mathbf x_r})\geq f({\mathbf x_n})$，即 ${\mathbf x_r}$ 为最差点，这意味着在 ${\mathbf x_{n+1}}$ 和 ${\mathbf x_{r}}$ 之间可能存在谷点，这时寻找收缩点（contraction point）：$${\mathbf x_c}={\mathbf x_0}+\rho({\mathbf x_{n+1}}-{\mathbf x_0})，$$通常取 $\rho=0.5$。
    - 如果 $f({\mathbf x_c}) < f({\mathbf x_{n+1}})$，那么用收缩点 ${\mathbf x_c}$ 形成新的单纯形。
    - 如果 $f({\mathbf x_c}) \geq f({\mathbf x_{n+1}})$，说明收缩无效，需要一个更小的单纯形，这时将 ${\mathbf x_{1}}$ 以外的点按以下方式压缩（shrink）：
    $${\mathbf x_i}={\mathbf x_1}+\sigma({\mathbf x_i}-{\mathbf x_1})，$$通常取 $\sigma = 0.5$，得到新的单纯形后返回第一步。
  


由此可见，Nelder Mead 算法只用到了函数值，不依赖于求导函数，是一种比较稳健的优化算法。图 \@ref(fig:nelder) 展示了 Nelder Mead 算法优化函数 $f(x_1, x_2) = x_1^2 + x_2^2 + x_1\sin(x_2) + x_2\sin(x_1)$。

```{r nelder, echo=FALSE, fig.cap="单纯形法优化动图"}
knitr::include_graphics("./rcode/nelder.gif")
```


#### 停止条件 {-}

由图 \@ref(fig:nelder) 可见，随着迭代过程，单纯形的体积越来越小，因此通常将单纯形的体积足够小做为Nelder Mead。有 $n+1$ 个顶点的单纯形的体积为： $$\frac{1}{2}|\mathrm{det}(\tilde{X})|,$$其中 $$\tilde{X}=\left({\mathbf x_1}-{\mathbf x_{n+1}},{\mathbf x_2-{\mathbf x_{n+1}}},\ldots, {\mathbf x_{n}-{\mathbf x_{n+1}}}\right).$$


在 R 中，Nelder Mead 算法可以用 `optim()` 实现。

## 随机梯度下降算法

### 梯度下降算法 {#gd}



梯度下降（Gradient Descent，或者 Steepest Descent）算法是一种迭代算法。对于一个多元连续可导函数 $f(\mathbf{x})$，在 $\mathbf{x}_n$ 处 $f(\mathbf{x})$下降最快的方向也就是$f(\mathbf{x})$ 在 $\mathbf{x}_n$ 处的梯度方向的反方向，即$-\nabla f(\mathbf{x}_n)$，这也是与 $\mathbf{x}_n$ 处等高线正交的方向。所以给定当前 $\mathbf{x}_n$，梯度下降算法的更新策略就是：
\begin{equation}
\mathbf{x}_{n+1}=\mathbf{x}_{n}-\alpha  \nabla f\left(\mathbf{x}_{n}\right),
(\#eq:gd)
\end{equation}

其中 $\alpha > 0$  控制下降步长。我们可以选择 $\alpha$ 以最小化：
\begin{equation}
g(\alpha) = f(\mathbf{x}_n - \alpha \nabla f(\mathbf{x}_n),
(\#eq:stepsize)
\end{equation}
如果我们的初始猜测为 $\mathbf{x}_0$，那么根据公式 \@ref(eq:gd)，我们可以得到 $f(\mathbf{x}_0) \geq f(\mathbf{x}_1) \geq f(\mathbf{x}_2) \geq \cdots$，如果算法收敛，序列 $\{\mathbf{x}_n\}$ 就会收敛到 $f(\mathbf{x})$ 的局部极小值。所以我们可以以 $f(\mathbf{x}_{n+1})- f(\mathbf{x}_n) \leq \epsilon$ 为梯度下降算法收敛的停止条件，其中 $\epsilon$ 为容忍值。

```{r gd-1, echo=FALSE, fig.cap='梯度下降算法：一元函数'}
library(ggplot2)
f <- function(x) x^2 - 5
gd.eg1 <- data.frame(x = 4:1, y = f(4:1))
ggplot(data.frame(x=c(-5, 5)), aes(x)) + 
  stat_function(fun=f) + 
  geom_path(data= gd.eg1, aes(x = x, y = y), arrow = arrow(), color = 'red')+
  geom_point(data = gd.eg1, aes(x = x, y = y), color = 'red', size = 2) + 
  ggtitle(parse(text = paste0(' ~ f(x) == ~ x^2 - 5'))) + xlab(expression(x)) +
  ylab(expression(f(x))) 
```


```{r gd, fig.cap='梯度下降算法：二元函数', fig.asp=1, echo=FALSE}
f <- function(x, y) { x^2 + y ^2}
dx <- function(x,y) {2*x }
dy <- function(x,y) {2*y}
num_iter <- 10
learning_rate <- 0.2
x_val <- 6
y_val <- 6

updates_x <- c(x_val, rep(NA, num_iter))
updates_y <- c(y_val, rep(NA, num_iter))
updates_z <- c(f(x_val, y_val), rep(NA, num_iter))

# parameter updates
for (i in 1:num_iter) {

  dx_val = dx(x_val,y_val)
  dy_val = dy(x_val,y_val)

  x_val <- x_val-learning_rate*dx_val
  y_val <- y_val-learning_rate*dy_val
  z_val <- f(x_val, y_val)

  updates_x[i + 1] <- x_val
  updates_y[i + 1] <- y_val
  updates_z[i + 1] <- z_val
}

## plotting
m <- data.frame(x = updates_x, y = updates_y)
x <- seq(-6, 6, length = 100)
y <- x
g <- expand.grid(x, y)
z <- f(g[,1], g[,2])
f_long <- data.frame(x = g[,1], y = g[,2], z = z)
library(ggplot2)
ggplot(f_long, aes(x, y, z = z)) + 
  geom_contour_filled(aes(fill = stat(level)), bins = 50) + 
  guides(fill = FALSE) +
  geom_path(data = m, aes(x, y, z=0), col = 2, arrow = arrow()) +
  geom_point(data = m, aes(x, y, z=0), size = 3, col = 2) +
  xlab(expression(x[1])) +
  ylab(expression(x[2])) +
  ggtitle(parse(text = paste0('~ f(x[1],x[2]) == ~ x[1]^2 + x[2]^2'))) 
```

值得注意的是，通常情况下梯度反方向迭代是符合逻辑的，但是当待优化函数中的某些参数存在高度相关关系时，梯度下降算法的收敛速度可能会降低。

```{r gd-cor, fig.cap='梯度下降算法（变量高度相关的情况）', fig.asp=1, echo=FALSE}
f <- function(x, y) { x^2 + y^2 - 1.6*x*y}
dx <- function(x,y) { (2*x - 1.6*y)}
dy <- function(x,y) { (2*y - 1.6*x)}
num_iter <- 20
learning_rate <- 0.42
x_val <- 0
y_val <- 4

updates_x <- c(x_val, rep(NA, num_iter))
updates_y <- c(y_val, rep(NA, num_iter))
updates_z <- c(f(x_val, y_val), rep(NA, num_iter))

# parameter updates
for (i in 1:num_iter) {

  dx_val = dx(x_val,y_val)
  dy_val = dy(x_val,y_val)

  x_val <- x_val-learning_rate*dx_val
  y_val <- y_val-learning_rate*dy_val
  z_val <- f(x_val, y_val)

  updates_x[i + 1] <- x_val
  updates_y[i + 1] <- y_val
  updates_z[i + 1] <- z_val
}

## plotting
m <- data.frame(x = updates_x, y = updates_y)
x <- seq(-5, 5, length = 100)
y <- seq(-5, 5, length = 100)
g <- expand.grid(x, y)
z <- f(g[,1], g[,2])
f_long <- data.frame(x = g[,1], y = g[,2], z = z)
library(ggplot2)
ggplot(f_long, aes(x, y, z = z)) + 
  geom_contour_filled(aes(fill = stat(level)), bins = 50) + 
  guides(fill = FALSE) +
  geom_path(data = m, aes(x, y, z=0), col = 2, arrow = arrow()) +
  geom_point(data = m, aes(x, y, z=0), size = 3, col = 2) +
  xlab(expression(x[1])) +
  ylab(expression(x[2])) +
  ggtitle(parse(text = paste0(' ~ f(x[1],x[2]) == ~ x[1]^2 + x[2]^2 - 1.6*x[1]*x[2]'))) 
```



```{example}
用梯度下降算法估计线性回归模型。
```

假设一般的线性回归模型 $y = X\beta + \eta,~\eta \sim N(0, I)$，给定 $n$ 个观测样本，我们可以写出线性模型的损失函数 
$$J(\beta) = \frac{1}{2n}(X\beta - y)^T(X\beta - y).$$ 对应的梯度向量为 
$$\nabla J(\beta) = \frac{1}{n}X^T(X\beta-y).$$

```{r}
gd.lm <- function(X, y, beta.init, alpha, tol = 1e-5, max.iter = 100){
  beta.old <- beta.init
  J <- betas <- list()
  if (alpha == 'auto'){
    alpha <-
      optim(0.1, function(alpha) {
        lm.cost(X, y, beta.old - alpha * lm.cost.grad(X, y, beta.old))
      }, method = 'L-BFGS-B', lower = 0, upper = 1)
    if (alpha$convergence == 0) {
      alpha <- alpha$par
    } else{
      alpha <- 0.1
    }
  }
  betas[[1]] <- beta.old
  J[[1]] <- lm.cost(X, y, beta.old)
  beta.new <- beta.old - alpha * lm.cost.grad(X, y, beta.old)
  betas[[2]] <- beta.new
  J[[2]] <- lm.cost(X, y, beta.new)
  iter <- 0
  while ((abs(lm.cost(X, y, beta.new) - lm.cost(X, y, beta.old))  > tol) & (iter < max.iter)){
    beta.old <- beta.new
    if (alpha == 'auto'){
    alpha <-
      optim(0.1, function(alpha) {
        lm.cost(X, y, beta.old - alpha * lm.cost.grad(X, y, beta.old))
      }, method = 'L-BFGS-B', lower = 0, upper = 1)
    if (alpha$convergence == 0) {
      alpha <- alpha$par
    } else{
      alpha <- 0.1
    }
    }
    beta.new <- beta.old - alpha * lm.cost.grad(X, y, beta.old)
    iter <- iter + 1
    betas[[iter + 2]] <- beta.new
    J[[iter + 2]] <- lm.cost(X, y, beta.new)
  }
  if (abs(lm.cost(X, y, beta.new) - lm.cost(X, y, beta.old)) > tol){
    cat('算法无法收敛 \n')
  } else{
    cat('算法收敛\n')
    cat('共迭代',iter + 1,'次','\n')
    cat('系数估计为：', beta.new, '\n')
    return(list(coef = betas,
                cost = J,
                niter = iter + 1))
  }
}
```




```{r}
## Generate some data
beta0 <- 1
beta1 <- 3
sigma <- 1
n <- 10000
x <- rnorm(n, 0, 1)
y <- beta0 + x * beta1 + rnorm(n, mean = 0, sd = sigma)
X <- cbind(1, x)

## Make the cost function
lm.cost <- function(X, y, beta){
   n <- length(y)
   loss <- sum((X%*%beta -  y)^2)/(2*n)
return(loss)
}
## Calculate the gradient
lm.cost.grad <- function(X, y, beta){
  n <- length(y)
  (1/n)*(t(X)%*%(X%*%beta - y))
}
## Use optimized alpha
gd.auto <- gd.lm(X, y, beta.init = c(-4,-5), alpha = 'auto', tol = 1e-5, max.iter = 10000)

## alpha = 0.1
gd1 <- gd.lm(X, y, beta.init = c(-4,-5), alpha = 0.1, tol = 1e-5, max.iter = 10000)

## alpha = 0.01
gd2 <- gd.lm(X, y, beta.init = c(-4,-5), alpha = 0.01, tol = 1e-5, max.iter = 10000)
```

```{r niter}
niter <- data.frame(alpha = c('auto', 0.1, 0.01), niter = c(gd.auto$niter, gd1$niter, gd2$niter))
knitr::kable(
  niter, caption = '不同 $\\alpha$ 对应的梯度下降算法的迭代次数',booktabs = TRUE,  col.names = c("$\\alpha$", '迭代次数'),
  
) 
```

从表~\@ref(tab:niter)~中我们可以发现在每次迭代过程中对 $\alpha$ 进行优化可以显著提高计算效率。如图~\@ref(fig:gd2)~，如果 $\alpha$ 较小，迭代的次数就会增加，因此收敛过程比较缓慢。

```{r gd2, echo = FALSE, fig.cap="不同 $\\alpha$ 取值下梯度下降算法的收敛情况", fig.asp=0.4}
library(dplyr)
library(reshape2)
gd1.betas <- as.data.frame(t(do.call(cbind, gd1$coef)))
gd2.betas <- as.data.frame(t(do.call(cbind, gd2$coef)))
colnames(gd1.betas) <- colnames(gd2.betas) <- c('beta0', 'beta1')
gd1.betas <- gd1.betas %>% mutate(Iteration = 1:nrow(gd1.betas)) %>% melt(id.vars = 'Iteration', variable.name = 'coef')
gd2.betas <- gd2.betas %>% mutate(Iteration = 1:nrow(gd2.betas)) %>% melt(id.vars = 'Iteration', variable.name = 'coef')
p1 <- ggplot(gd1.betas, aes(Iteration, value)) + 
  geom_line(aes(colour = coef)) + ggtitle(bquote(alpha == 0.1)) + xlim(c(0,500)) +
  scale_colour_discrete(name = '', labels = expression(beta[0],beta[1]))
p2 <- ggplot(gd2.betas, aes(Iteration, value)) + 
  geom_line(aes(colour = coef)) + ggtitle(bquote(alpha == 0.01)) + xlim(c(0,500)) +
  scale_colour_discrete(name = '', labels = expression(beta[0],beta[1]))
library(patchwork)
p1+p2
```


### 随机梯度下降算法

梯度下降算法通常收敛速度较慢，因为在每次迭代中对每个样本值都需要计算梯度，在样本量较大时计算量会随之增大。随机梯度下降算法的思想是在算法的每次迭代更新时只考虑一个样本。以线性回归模型为例，就是讲梯度下降算法中的 $$\nabla J(\beta) = \frac{1}{n}X^T(X\beta-y).$$ 替换为 ：
$$\nabla J(\beta)_i = X_i^T(X_i\beta-y_i),$$ 完整的迭代过程如下。


\centering
```{block, type='method'}
**随机梯度下降算法**

1. 将训练数据随机排序，然后重复第2步直至收敛。
2. 抽取一个随机样本 $i\in\{1, 2, \cdots, n\}$，根据下式对 $\beta$
进行迭代。$$\beta:= \beta - \alpha \nabla J(\beta)_i.$$
```

随机梯度下降在R中的实现如下。

```{r}
sgd.lm <- function(X, y, beta.init, alpha = 0.5, n.samples = 1, tol = 1e-5, max.iter = 100){
  n <- length(y)
  beta.old <- beta.init
  J <- betas <- list()
  sto.sample <- sample(1:n, n.samples, replace = TRUE)
  betas[[1]] <- beta.old
  J[[1]] <- lm.cost(X, y, beta.old)
  beta.new <- beta.old - alpha * sgd.lm.cost.grad(X[sto.sample, ], y[sto.sample], beta.old)
  betas[[2]] <- beta.new
  J[[2]] <- lm.cost(X, y, beta.new)
  iter <- 0
  n.best <- 0
  while ((abs(lm.cost(X, y, beta.new) - lm.cost(X, y, beta.old))  > tol) & (iter + 2 < max.iter)){
    beta.old <- beta.new
    sto.sample <- sample(1:n, n.samples, replace = TRUE)
    beta.new <- beta.old - alpha * sgd.lm.cost.grad(X[sto.sample, ], y[sto.sample], beta.old)
    iter <- iter + 1
    betas[[iter + 2]] <- beta.new
    J[[iter + 2]] <- lm.cost(X, y, beta.new)
  }
  if (abs(lm.cost(X, y, beta.new) - lm.cost(X, y, beta.old)) > tol){
    cat('算法无法收敛。 \n')
  } else{
    cat('算法收敛\n')
    cat('共迭代',iter + 1,'次','\n')
    cat('系数估计为：', beta.new, '\n')
    return(list(coef = betas,
                cost = J,
                niter = iter + 1))
  }
}

## Make the cost function
sgd.lm.cost <- function(X, y, beta){
   n <- length(y)
   if (!is.matrix(X)){
    X <- matrix(X, nrow = 1)
  }
   loss <- sum((X%*%beta -  y)^2)/(2*n)
return(loss)
}
## Calculate the gradient
sgd.lm.cost.grad <- function(X, y, beta){
  n <- length(y)
  if (!is.matrix(X)){
    X <- matrix(X, nrow = 1)
  }
  t(X)%*%(X%*%beta - y)/n
}

```

现在我们在章节~\@ref(gd)~中模拟的线性回归数据中实现SGD算法。

```{r sgdlm, echo=FALSE, fig.cap='随机梯度下降算法的收敛情况'}
# test on the generated data
sgd.est <- sgd.lm(X, y, beta.init = c(-4,-5), alpha = 0.05, tol = 1e-5, max.iter = 10000)
betas <- as.data.frame(t(do.call(cbind,sgd.est$coef)))
colnames(betas) <- c('beta0', 'beta1')
betas <- as.data.frame(betas) %>% mutate(Iteration = 1:nrow(betas)) %>% melt(id.vars = 'Iteration', variable.name = 'coef')
ggplot(betas, aes(Iteration, value)) + 
  geom_line(aes(colour = coef)) +
  scale_colour_discrete(name = '', labels = expression(beta[0],beta[1]))
```

## 优化算法比较



```{r nt, results='hide', echo=FALSE}
# newton method
func = function(beta) {
    sum((y - beta[1] - beta[2] * x)^2)/2/length(y)
}
grad = function(beta) {
    matrix(c(sum(-2 * (y - beta[1] - beta[2] * x)), sum(-2 * x * (y - beta[1] - 
        beta[2] * x))), 2, 1)/length(y)
}
hess = function(beta) {
    matrix(c(2 * length(x), 2 * sum(x), 2 * sum(x), 2 * sum(x^2)), 2, 2)/length(y)
}
newton <- function(f3, x0, tol = 1e-9, n.max = 100) {
  # Newton's method for optimisation, starting at x0
  # f3 is a function that given x returns the list
  # {f(x), grad f(x), Hessian f(x)}
  
  x <- x0
  f3.x <- f3(x)
  xs <- list()
  xs[[1]] <- x
  n <- 0
  while((max(abs(f3.x[[2]])) > tol) & (n < n.max)){
    x <- x - solve(f3.x[[3]], f3.x[[2]])
    f3.x <- f3(x)
    n <- n + 1
    xs[[n + 1]] <- x
  }
  if (n == n.max){
    cat('newton failed to converge. \n')
  } else{
    cat('iterated', n+1, 'times. \n')
    return(xs)
  }
}
## Newton optimization
optimOut <- newton(function(beta){list(func(beta), grad(beta), hess(beta))}, c(-4, -5))  
```

```{r optimcomp, fig.asp=0.8, echo=FALSE, fig.cap='梯度下降算法、随机梯度下降算法和牛顿算法的收敛情况比较'}
trace.newton <- data.frame(x = sapply(optimOut, `[`, 1), y = sapply(optimOut, `[`, 2))
trace.gd <- data.frame(x = sapply(gd1$coef, `[`, 1), y = sapply(gd1$coef, `[`, 2))
trace.sgd <- data.frame(x = sapply(sgd.est$coef, `[`, 1), y = sapply(sgd.est$coef, `[`, 2))
xs <- seq(-5, 5, length = 100)
ys <- seq(-5, 5, length = 100)
g <- expand.grid(xs, ys)
z <- sapply(1:dim(g)[1], function(i){lm.cost(X, y, c(g[i,1],g[i,2]))})
f_long <- data.frame(x = g[,1], y = g[,2], z = z)
colors <- c("SGD" = "blue", "Newton" = "red", "GD" = "green")

ggplot(f_long, aes(x, y, z = z)) + 
  geom_contour_filled(aes(fill = stat(level)), bins = 200) + 
  guides(fill = FALSE) +
  geom_path(data = trace.sgd, aes(x, y, z=0, color = 'SGD'), arrow = arrow(), alpha = 0.5) +
  geom_point(data = trace.sgd, aes(x, y, z=0, color = 'SGD'), size = 1.1, alpha = 0.5) +
  geom_path(data = trace.newton, aes(x, y, z=0, color = 'Newton'), arrow = arrow(), alpha = 0.5) +
  geom_point(data = trace.newton, aes(x, y, z=0, color = 'Newton'), size = 1.1, alpha = 0.5) +
  geom_path(data = trace.gd, aes(x, y, z=0, color = 'GD'), arrow = arrow(), alpha = 0.5) +
  geom_point(data = trace.gd, aes(x, y, z=0, color = 'GD'), size = 1.1, alpha = 0.5) +
  ggtitle('') +
  labs(x = expression(beta[0]),
         y = expression(beta[1]),
         color = "Method") +
    scale_color_manual(values = colors)
```

基于章节~\@ref(gd)~中模拟的线性回归数据，我们来比较梯度下降算法、随机梯度下降算法和牛顿算法。从图~\@ref(fig:optimcomp)~中我们可以发现：

1. 牛顿迭代的收敛速度很快，前提条件是海塞矩阵存在，但在复杂的机器学习方法中海塞矩阵的解析表达式通常很难求得或者无法求得。

2. 如果训练样本的个数**非常**庞大，牛顿方法或者梯度下降方法可能会花费很长的时间才可以收敛，这时随机梯度下降是一个更好的选择。

3. 随机梯度下降的收敛路径带有较多噪音，这是因为SGD的每次迭代只用了一个随机样本，为了平衡SGD带来的不确定性和GD的计算复杂度，我们可以在每步迭代时抽取少量样本，通常可以达到更加稳健的收敛，这也就是 mini-batch GD。

4. 对于 $\alpha$ 的选择，可以参考 @bottou2012stochastic 和 @zeiler2012adadelta。


## 拓展阅读 {#optim-reading}


* @bottou2012stochastic 是实战中随机梯度下降算法通常用到的一些技巧。
* @zeiler2012adadelta 是关于如何适应性的选择梯度下降算法和随机梯度下降算法中的 $\alpha$。


