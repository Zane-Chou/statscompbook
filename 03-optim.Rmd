# 常见的优化算法 {#optim}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
library(tidyverse) # Wickham的数据整理的整套工具
pdf.options(height=10/2.54, width=10/2.54, family="GB1") # 注意：此设置要放在最后
```

## 介绍 {#optim-intro}



## 牛顿优化算法

假设函数 $f(x)$ 可导并一阶导 $f^{\prime}(x)$ 连续，那么最大化 $f(x)$ 相当于求 $f^{\prime}(x) = 0$ 的根，我们就可以使用牛顿法求根。优化 $f(x)$ 就转化成了方程求根问题。 牛顿优化算法的迭代过程如下。

\centering
```{block, type='method'}
**牛顿优化算法**


1. 选择初始猜测点 $x_0$，设置 $n = 0$。
2. 按照以下迭代过程进行迭代：
\begin{equation}
x_{n+1}=x_{n}-\frac{f^{\prime}\left(x_{n}\right)}{f^{\prime\prime}\left(x_{n}\right)}.
\end{equation}
3. 计算 $|f^{\prime}(x_{n+1})|$。
    3a. 如果 $|f^{\prime}(x_{n+1})| \leq \epsilon$，停止迭代；
    3b. 否则，返回第 2 步。
```

上述优化过程的停止条件还可以为：

- $|f^{'}(x_{n+1})| < \epsilon$.

- $|x_{n} - x_{n-1}| < \epsilon$.

- $|f(x_{n}) - f(x_{n-1})| < \epsilon$.

现在我们将牛顿优化算法在 R 中实现，具体请见函数 `newton()` 。

```{r newton}

```

