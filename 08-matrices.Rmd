# 现代线性代数方法 {#matrices}
 
矩阵无处不在，比如任何可以放在 Excel 电子表格中的东西都是矩阵，文本和图像也可以用矩阵表示，线性回归的估计可以通过矩阵运算得到，谷歌的页面排序是通过特征分解来计算的。随着 5G 时代的到来及数据的爆发式增长，传统的线性代数方法很难完成大规模计算，因此，了解**矩阵计算的数值算法**及其应用，可以帮助提供更高效的解决方案。例如，近似矩阵计算通常比精确矩阵计算快几千倍。[20世纪的十大科学与工程算法](http://pi.math.cornell.edu/~web6140/)就包括矩阵分解方法和 QR 算法（我们将要在后续章节中介绍）。


##  特征分解

### 特征分析

一般来讲，一个 $n \times n$ 的矩阵 $A$ 乘以一个向量 $x$ 之后得到的向量 $Ax$ 既改变 $x$ 的长度，也改变其方向。但是，存在某些向量 $x$，使得 $Ax$ 只改变 $x$ 的长度而不改变其方向：即满足 $Ax = \lambda x$，那么 $\lambda$ 是 $A$ 的特征值，$x$ 是其特征向量。在二维情况下的特征分析（eigenanalysis）过程如图~\@ref(fig:eigen)~所示^[来源：https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors]。

```{r eigen, fig.cap='二维矩阵的特征分析', echo=FALSE, fig.align='center'}
knitr::include_graphics('figs/eigen.png')
```

```{example}
**谷歌页面排序（PageRank）**


```

### 矩阵的特征分解过程

特征分解（eigendecomposition）又叫谱分解（Spectral decomposition），是把一个矩阵根据其特征值和特征向量分解的过程。只有可以正交化的矩阵才可以进行特征分解。

如果 $n \times n$ 的矩阵 $A$ 具有 $n$ 个线性独立的特征向量，那么 $A$ 是可以正交化的，此时 $A$ 的特征分解为：

$$A = Q \Lambda Q^{-1},$$
其中 $Q$ 是一个 $n \times n$ 的方阵，其第 $i$ 列对应 $A$ 的第 $i$ 个特征向量 $q_{i}$，$\Lambda$ 是一个对角阵，其对角线上的元素对应 $A$ 的 $n$ 个特征值：$\Lambda_{ii} = \lambda_{i}$。

注意：只有可以正交化的矩阵才可以进行上述特征分解。

如果 $A$ 是一个 $n \times n$ 的实对称阵，那么

1. $A$ 的 $n$ 个特征根都为实数。
2. $A$ 有 $n$ 个互相正交的特征向量。

因此，一个实对称矩阵 $A$ 是可以正交化的，也就可以进行特征分解。

如果矩阵 $A$ 不可以对角化，那么可以通过 Schur 分解将其进行三角化：

$$A = QUQ^{-1},$$
其中 $Q$ 是一个正交阵，$U$ 是一个上三角矩阵，其对角线上的元素对应 $A$ 的特征值。



### 特征分解可以帮助我们做什么？

#### 矩阵快速求逆 {-}

根据非奇异方阵 $A$ 特征分解的过程，我们很容易可以得到：

$$A^{-1} = Q\Lambda^{-1} Q^{-1},$$
因为 $\Lambda$ 是对角阵，它的逆 $\Lambda^{-1}$ 也是对角阵。

#### 矩阵快速幂运算 {-}

特征分解可以帮助简化矩阵的幂运算，假设 $f(x) = a_0 + a_1x + a_2x^2$，我们可以得到：

$$f(A) = Qf(\Lambda)Q^{-1},$$
因为 $\Lambda$ 是对角阵，$f(\Lambda)$ 也是对角阵，其对角线上的元素 $[f(\Lambda)]_{ii} = f(\lambda_i)$。据此，我们很容易就可以计算矩阵 $A$ 的高次方。以 $A^{20}$ 为例：

$$A^{20} = Q\Lambda^{20}Q^{-1}.$$


## 奇异值分解

## 数值方法



